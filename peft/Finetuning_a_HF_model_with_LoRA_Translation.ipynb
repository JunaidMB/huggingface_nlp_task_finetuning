{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8KlhDLxBQIG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune a T5 model to translate from English to French\n",
        "\n",
        "Task Description: We will finetune the Google T5 transformer model to translate from English to French. The dataset used is a subset of the OPUS Books.\n",
        "\n",
        "Original Tutorial: https://huggingface.co/docs/transformers/tasks/translation"
      ],
      "metadata": {
        "id": "uswoCudXS9d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq transformers datasets evaluate accelerate bitsandbytes loralib peft"
      ],
      "metadata": {
        "id": "aCHn6F_0S91Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the OPUS Books Dataset"
      ],
      "metadata": {
        "id": "VgubDQfxTdVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "books = load_dataset(\"opus_books\", \"en-fr\", split = 'train[0:5000]')"
      ],
      "metadata": {
        "id": "voHsYYPDTfvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into a train and test set\n",
        "books = books.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "OZz3FRvJTpEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the data\n",
        "import pprint\n",
        "pprint.pprint(books['train'][1])\n",
        "\n",
        "# The translation column is our model input\n"
      ],
      "metadata": {
        "id": "Agi57GyYT0t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "## Load Model\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "8uG8-vV0UE3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a PEFT Model"
      ],
      "metadata": {
        "id": "KLpKFpOvvD45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "\n",
        "# Create a config corresponding to the PEFT method\n",
        "peft_config = LoraConfig(\n",
        "    task_type = TaskType.SEQ_2_SEQ_LM,\n",
        "    inference_mode = False,\n",
        "    #target_modules= [\"\"],\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "ZJ5roUYtvECU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap base model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "model = get_peft_model(model, peft_config)\n"
      ],
      "metadata": {
        "id": "ZWsQeqYsvEHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "dsK-hU0svEM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "Ts0MjPTtvEXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "We need to create a preprocessing function that performs the following operations:\n",
        "\n",
        "1. Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
        "\n",
        "2. Tokenize the input (English) and target (French) separately because you can’t tokenize French text with a tokenizer pretrained on an English vocabulary.\n",
        "\n",
        "3. Truncate sequences to be no longer than the maximum length set by the max_length parameter."
      ],
      "metadata": {
        "id": "Rfza1skhXCja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = \"en\"\n",
        "target_lang = \"fr\"\n",
        "prefix = \"translate English to French: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "  inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
        "  targets = [example[target_lang] for example in examples['translation']]\n",
        "  model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "oTAkSinHXVeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Operation to apply to every instance\n",
        "tokenized_books = books.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "p0sgMD-jX9wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a batch of examples, with dynamic padding. Use the appropriate collator function\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ],
      "metadata": {
        "id": "mKaGfbk0YhfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate\n",
        "\n",
        "We want to create a `compute_metrics` function that monitors a metric during training. For this task, use the SacreBLEU metric."
      ],
      "metadata": {
        "id": "acoBxUEtYpeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "8TxIjg0NZBit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "AFKLeNirX_Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ],
      "metadata": {
        "id": "N31KFECTY9n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train using the Trainer API\n",
        "The main training steps are:\n",
        "\n",
        "1. Define training hyperparameters using a model specific TrainingArguments function. At the end of each epoch, the Trainer will evaluate the defined loss metric and save the training checkpoint.\n",
        "\n",
        "2. Pass the training arguments to a Trainer function alongside the model, dataset, tokenizer, data collator.\n",
        "\n",
        "3. Call train() to finetune the model"
      ],
      "metadata": {
        "id": "U6uf-r9SZQRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n"
      ],
      "metadata": {
        "id": "Ud9oF3M7ZQ0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"my_awesome_opus_books_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=2,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_books[\"train\"],\n",
        "    eval_dataset=tokenized_books[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "QUCM7tAYZTNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the fine tuned model and obtain the perplexity score\n",
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"sacrebleu: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "RZeUUW9MZfj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"t5_translator\")"
      ],
      "metadata": {
        "id": "DFxrt4I3Zj12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this case, the tokenizer was not saved automatically, save it manually in the model folder for inference\n",
        "tokenizer.save_pretrained(\"t5_translator\", legacy_format=False)"
      ],
      "metadata": {
        "id": "vnpuNBKLZqTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "\n",
        "Use model for inference using a pipeline wrapper"
      ],
      "metadata": {
        "id": "oMWkWzXtZqmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_id = \"t5_translator\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "0joby4nnwKOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model = model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "8-Xw3VjUwJ7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\""
      ],
      "metadata": {
        "id": "v0hLAdfrZtIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Pipeline using Pytorch\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5_translator\")\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model.generate(input_ids = inputs.to(device), max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)"
      ],
      "metadata": {
        "id": "mbU1Ng2LZ8NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "6FpcFxk0aWUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "37QacEAnzdi-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}