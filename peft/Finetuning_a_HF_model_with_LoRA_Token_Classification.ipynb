{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "collapsed_sections": [
        "n_Zmee_6m_RY",
        "1HRsipJYrZE8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune a DistilBERT model on the WNUT 17 dataset"
      ],
      "metadata": {
        "id": "9O3U9EIMllTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq transformers datasets evaluate seqeval accelerate bitsandbytes loralib peft"
      ],
      "metadata": {
        "id": "Xg1KM0aUlu-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load WNUT17 Dataset from the Datasets library"
      ],
      "metadata": {
        "id": "OKrTb3rWmCrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "wnut = load_dataset(\"wnut_17\")\n",
        "\n",
        "print(wnut)"
      ],
      "metadata": {
        "id": "SGpxMHKjl1qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the data\n",
        "print(wnut['train'][0])"
      ],
      "metadata": {
        "id": "SbjgK-NcmR0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each number in ner_tags column represents an entity. We can convert the numbers to names to get labels\n",
        "label_list = wnut[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "label_list"
      ],
      "metadata": {
        "id": "Gy1er_LqmZVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The letter that prefixes each ner_tag indicates the token position of the entity:\n",
        "\n",
        "B- indicates the beginning of an entity.\n",
        "I- indicates a token is contained inside the same entity (for example, the State token is a part of an entity like Empire State Building).\n",
        "0 indicates the token doesnâ€™t correspond to any entity."
      ],
      "metadata": {
        "id": "fbvW_lx3m59p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "ZCVnGDWK27K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at an example instance\n",
        "import pprint\n",
        "example = wnut[\"train\"][0]\n",
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "pprint.pprint(example)\n",
        "pprint.pprint( example['tokens'])\n",
        "\n",
        "pprint.pprint(tokenized_input)\n",
        "pprint.pprint(tokenized_input['input_ids'])\n",
        "pprint.pprint(tokens)"
      ],
      "metadata": {
        "id": "DF-nVB0q2-fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a PEFT Model"
      ],
      "metadata": {
        "id": "Lwy2TMbR3AAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "\n",
        "# Create a config corresponding to the PEFT method\n",
        "peft_config = LoraConfig(\n",
        "    task_type = TaskType.TOKEN_CLS,\n",
        "    target_modules= [\"classifier\"],\n",
        "    inference_mode = False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "MTBNZ7Rd3CYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before training, we have to create a map of expected ids to labels with id2label and label2id\n",
        "id2label = {\n",
        "    0: \"O\",\n",
        "    1: \"B-corporation\",\n",
        "    2: \"I-corporation\",\n",
        "    3: \"B-creative-work\",\n",
        "    4: \"I-creative-work\",\n",
        "    5: \"B-group\",\n",
        "    6: \"I-group\",\n",
        "    7: \"B-location\",\n",
        "    8: \"I-location\",\n",
        "    9: \"B-person\",\n",
        "    10: \"I-person\",\n",
        "    11: \"B-product\",\n",
        "    12: \"I-product\",\n",
        "}\n",
        "label2id = {\n",
        "    \"O\": 0,\n",
        "    \"B-corporation\": 1,\n",
        "    \"I-corporation\": 2,\n",
        "    \"B-creative-work\": 3,\n",
        "    \"I-creative-work\": 4,\n",
        "    \"B-group\": 5,\n",
        "    \"I-group\": 6,\n",
        "    \"B-location\": 7,\n",
        "    \"I-location\": 8,\n",
        "    \"B-person\": 9,\n",
        "    \"I-person\": 10,\n",
        "    \"B-product\": 11,\n",
        "    \"I-product\": 12,\n",
        "}"
      ],
      "metadata": {
        "id": "hb0oBa3u4Cnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap base model\n",
        "model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=len(id2label), id2label= id2label, label2id = label2id)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n"
      ],
      "metadata": {
        "id": "GJ1uACJ03CbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "r9QJ5Fes3CeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "pWfplxyR3CgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "We want to preprocess this data which are tweets. The operations for preprocessing are:\n",
        "\n",
        "1. Mapping all tokens to their corresponding word with the `word_ids` method\n",
        "\n",
        "2. Assigning the label `-100` to the special tokens `[CLS]` and `[SEP]` so they're ignored by the PyTorch loss function\n",
        "\n",
        "3. Only label the first token of a given word. Assign `-100` to other subtokens from the **same** word"
      ],
      "metadata": {
        "id": "n_Zmee_6m_RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocess function\n",
        "def tokenize_and_align_labels(examples):\n",
        "  tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "  labels = []\n",
        "  for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "    word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "      if word_idx is None:\n",
        "        label_ids.append(-100)\n",
        "      elif word_idx != previous_word_idx:\n",
        "        # Only label the first token of a given word\n",
        "        label_ids.append(label[word_idx])\n",
        "      else:\n",
        "        label_ids.append(-100)\n",
        "\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "\n",
        "\n",
        "  tokenized_inputs[\"labels\"] = labels\n",
        "\n",
        "  return tokenized_inputs"
      ],
      "metadata": {
        "id": "-sdDyYaFoO-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to every instance in the dataset\n",
        "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched = True,)"
      ],
      "metadata": {
        "id": "2zEGex3OpuZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a batch of examples, with dynamic padding. Use the appropriate collator function\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "0yTi6GvUrMzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate\n",
        "We require monitoring a metric during training to see how well our model is doing. We use the evaluate library to load an evaluation metric - we use the seqeval metric. seqeval produces precision, recall, F1 score and accuracy.\n",
        "\n",
        "With the loss metric defined, we must define a function that takes model predictions and labels and computes the loss metric. This is usually called the compute_metrics function."
      ],
      "metadata": {
        "id": "1HRsipJYrZE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "RVF5UQ8orU6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
        "\n",
        "def compute_metrics(p):\n",
        "  predictions, labels = p\n",
        "  predictions = np.argmax(predictions, axis = 2)\n",
        "\n",
        "  true_predictions = [\n",
        "      [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "      for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "  true_labels = [\n",
        "      [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "      for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "\n",
        "  results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "  return {\n",
        "      \"precision\": results[\"overall_precision\"],\n",
        "      \"recall\": results[\"overall_recall\"],\n",
        "      'f1': results[\"overall_f1\"],\n",
        "      \"accuracy\": results[\"overall_accuracy\"],\n",
        "  }"
      ],
      "metadata": {
        "id": "PFz5uX0I4K8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train using the Trainer API\n",
        "\n",
        "The main training steps are:\n",
        "\n",
        "1. Define training hyperparameters using a model specific `TrainingArguments` function. At the end of each epoch, the Trainer will evaluate the defined loss metric and save the training checkpoint.\n",
        "\n",
        "2. Pass the training arguments to a Trainer function alongside the **model**, **dataset**, **tokenizer**, **data collator** and **compute metrics**\n",
        "\n",
        "3. Call train() to finetune the model"
      ],
      "metadata": {
        "id": "MzCUJ4u45VgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n"
      ],
      "metadata": {
        "id": "F92qA1sK6rEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"token_classification_wnut\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_wnut[\"train\"],\n",
        "    eval_dataset=tokenized_wnut[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "3gcGsxfC5evW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"token_classification_wnut_model\")"
      ],
      "metadata": {
        "id": "iMJtDy4v831S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "\n"
      ],
      "metadata": {
        "id": "skuuaBjF8gsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_id = \"token_classification_wnut_model\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=len(id2label), id2label= id2label, label2id = label2id)\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "yoYd8Aqm4wyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model = model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "1PXMbeFH43E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Michael Faraday was a scientist who lived in England in the 19th century.\""
      ],
      "metadata": {
        "id": "W2MJh6ax5cL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw Pytorch approach\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "## Tokenize inputs\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"token_classification_wnut_model\")\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "## Feed inputs to the model and return logits\n",
        "with torch.no_grad():\n",
        "  logits = model(**inputs.to(device)).logits"
      ],
      "metadata": {
        "id": "MgxKDIxl9GFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = torch.argmax(logits, dim=2)\n",
        "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
        "predicted_token_class"
      ],
      "metadata": {
        "id": "8GjBfuLf-rQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RzfM4Fo9AQ-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}