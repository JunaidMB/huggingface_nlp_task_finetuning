{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18pk78C1u2-3DsvG5E3XsVZfHDYBfp5J4","timestamp":1690982780216}],"gpuType":"T4","authorship_tag":"ABX9TyNQ/U01Qf5le76ONVfwXYx9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Finetune a DistilRoBERTa model on the ELI5 Dataset\n","\n","Task Description: Masked Language Modelling (MLM) predicts a masked token in a sequence. If we give it a sequence where some tokens inside are masked, it can predict the masked tokens.\n","\n","Original Tutorial: https://huggingface.co/docs/transformers/tasks/masked_language_modeling"],"metadata":{"id":"-YbIBXcexEdo"}},{"cell_type":"code","source":["!pip install -q transformers datasets evaluate accelerate"],"metadata":{"id":"rfiVGwyMxKEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load ELI5 dataset"],"metadata":{"id":"ocHWFEgMxUmn"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","eli5 = load_dataset(\"eli5\", split = \"train_asks[:5000]\")"],"metadata":{"id":"geYs0U6JxRxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the dataset into a train and test set\n","eli5 = eli5.train_test_split(test_size=0.2)"],"metadata":{"id":"ZHO-hefIxqYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Look at the data\n","import pprint\n","pprint.pprint(eli5['train'][0])\n","\n","# The text column is our model input\n"],"metadata":{"id":"OtcEya7Kx4-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing\n","## Load Model\n","from transformers import AutoTokenizer\n","\n","checkpoint = \"distilroberta-base\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"],"metadata":{"id":"IbtvEgvlyEbI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing\n","We need to create a preprocess function that we will apply to every instance in the dataset. The preprocess function needs to:\n","\n","1. Flatten the instance so that the text column is easily accessible\n","2. Join any list of strings\n","3. Tokenize result\n","\n","Some token sequences will be **longer** than the maximum input length for the model. Hence we use a second preprocessing function to:\n","\n","1. concatenate all token sequences\n","2. Split the concatenated sequences into shorter chunks defined by a `block_size` parameter."],"metadata":{"id":"sG9COA2JytEJ"}},{"cell_type":"code","source":["# The text field is nested so we need to flatten each instance\n","eli5 = eli5.flatten()\n","pprint.pprint(eli5['train'][0])"],"metadata":{"id":"eTKQ9klvyWSt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Operation to apply to every instance\n","print(\" \".join(eli5['train']['answers.text'][0]), \"\\n\")\n","print(tokenizer(\" \".join(eli5['train']['answers.text'][0])) )"],"metadata":{"id":"pnIBUZlxyj9C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wrap in a preprocess function\n","def preprocess_function(examples):\n","  return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])"],"metadata":{"id":"9lSTAYuqzKCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply preprocessing over entire dataset - batched = True process multiple elements of the datasets\n","tokenized_eli5 = eli5.map(preprocess_function, batched = True, num_proc=4, remove_columns=eli5['train'].column_names)"],"metadata":{"id":"4pYKGGO7zzYM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_eli5['train'][0]"],"metadata":{"id":"kBMLy0Jf0oHo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def group_texts(examples, block_size: int = 128):\n","  # This function is to cut the length of the text examples\n","\n","  # Concatencate all texts\n","  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","  total_length = len(concatenated_examples[list(examples.keys())[0]])\n","\n","  if total_length >= block_size:\n","    total_length = (total_length // block_size) * block_size\n","  # Split by chunks of block size\n","  result = {\n","      k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n","      for k, t in concatenated_examples.items()\n","  }\n","  result[\"labels\"] = result[\"input_ids\"].copy()\n","  return result\n"],"metadata":{"id":"PzzNf8pl0dz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply second preprocessing over entire dataset\n","lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"],"metadata":{"id":"QeuRft8M1khW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pprint.pprint(lm_dataset['train'][0])"],"metadata":{"id":"2J99xgzb1tH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a batch of examples, with dynamic padding. Use the appropriate collator function\n","from transformers import DataCollatorForLanguageModeling\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm_probability = 0.15)"],"metadata":{"id":"y-3cvRzy1zuF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train using the Trainer API\n","The main training steps are:\n","\n","1. Define training hyperparameters using a model specific TrainingArguments function. At the end of each epoch, the Trainer will evaluate the defined loss metric and save the training checkpoint.\n","\n","2. Pass the training arguments to a Trainer function alongside the model, dataset, tokenizer, data collator.\n","\n","3. Call train() to finetune the model"],"metadata":{"id":"BjcuQs822Wkv"}},{"cell_type":"code","source":["from transformers import AutoModelForMaskedLM, TrainingArguments, Trainer\n","\n","model = AutoModelForMaskedLM.from_pretrained(checkpoint)"],"metadata":{"id":"nNB5hINQ3MLe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir = \"eli5_mlm\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=2,\n","    fp16=True,\n","    push_to_hub=False,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=lm_dataset[\"train\"],\n","    eval_dataset=lm_dataset[\"test\"],\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()"],"metadata":{"id":"JInJ6PAd2XyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the fine tuned model and obtain the perplexity score\n","import math\n","\n","eval_results = trainer.evaluate()\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"],"metadata":{"id":"xbqxh3bN4TeR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(\"eli5_masked_model\")"],"metadata":{"id":"nm1XQNnI4g6I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# In this case, the tokenizer was not saved automatically, save it manually in the model folder for inference\n","tokenizer.save_pretrained(\"eli5_masked_model\", legacy_format=False)"],"metadata":{"id":"MYYZQz3E7oKV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference\n","\n","Use model for inference using a pipeline wrapper"],"metadata":{"id":"OXjM0h0C4s_F"}},{"cell_type":"code","source":["text = \"The Milky Way is a <mask> galaxy\""],"metadata":{"id":"HY2_zVlo4xy3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","mask_filler = pipeline(\"fill-mask\", model = \"eli5_masked_model\")\n","mask_filler(text, top_k = 3)"],"metadata":{"id":"8brWInxW486o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inference Pipeline using Pytorch\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"eli5_masked_model\")\n","inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n","mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n","\n","model = AutoModelForMaskedLM.from_pretrained(\"eli5_masked_model\")\n","logits = model(**inputs).logits\n","mask_token_logits = logits[0, mask_token_index, :]\n","\n","top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n","\n","for token in top_3_tokens:\n","  print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))"],"metadata":{"id":"NjkHHQvn5Fr5","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Decode the generated token ids back into text\n","tokenizer.batch_decode(outputs, skip_special_tokens=True)"],"metadata":{"id":"nvRw8bBb5qA9"},"execution_count":null,"outputs":[]}]}