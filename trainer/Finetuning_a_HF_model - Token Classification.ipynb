{"cells":[{"cell_type":"markdown","metadata":{"id":"9O3U9EIMllTR"},"source":["# Finetune a DistilBERT model on the WNUT 17 dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11797,"status":"ok","timestamp":1691083741810,"user":{"displayName":"Junaid Butt","userId":"08995823544923013890"},"user_tz":-60},"id":"Xg1KM0aUlu-z"},"outputs":[],"source":["!pip install -qqq transformers datasets evaluate seqeval accelerate"]},{"cell_type":"markdown","metadata":{"id":"OKrTb3rWmCrT"},"source":["# Load WNUT17 Dataset from the Datasets library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGpxMHKjl1qA"},"outputs":[],"source":["from datasets import load_dataset\n","\n","wnut = load_dataset(\"wnut_17\")\n","\n","print(wnut)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbjgK-NcmR0z"},"outputs":[],"source":["# Look at the data\n","print(wnut['train'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1097,"status":"ok","timestamp":1691085337974,"user":{"displayName":"Junaid Butt","userId":"08995823544923013890"},"user_tz":-60},"id":"Gy1er_LqmZVQ","outputId":"6782bd5d-b6e7-4b81-ae5a-eafcf3f214ef"},"outputs":[],"source":["# Each number in ner_tags column represents an entity. We can convert the numbers to names to get labels\n","label_list = wnut[\"train\"].features[\"ner_tags\"].feature.names\n","\n","label_list"]},{"cell_type":"markdown","metadata":{"id":"fbvW_lx3m59p"},"source":["The letter that prefixes each ner_tag indicates the token position of the entity:\n","\n","B- indicates the beginning of an entity.\n","I- indicates a token is contained inside the same entity (for example, the State token is a part of an entity like Empire State Building).\n","0 indicates the token doesnâ€™t correspond to any entity."]},{"cell_type":"markdown","metadata":{"id":"n_Zmee_6m_RY"},"source":["# Preprocessing\n","\n","We want to preprocess this data which are tweets. The operations for preprocessing are:\n","\n","1. Mapping all tokens to their corresponding word with the `word_ids` method\n","\n","2. Assigning the label `-100` to the special tokens `[CLS]` and `[SEP]` so they're ignored by the PyTorch loss function\n","\n","3. Only label the first token of a given word. Assign `-100` to other subtokens from the **same** word"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQKy1ErYm-mz"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L-5CG71lmwgm"},"outputs":[],"source":["# Look at an example instance\n","import pprint\n","example = wnut[\"train\"][0]\n","tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n","tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n","\n","pprint.pprint(example)\n","pprint.pprint( example['tokens'])\n","\n","pprint.pprint(tokenized_input)\n","pprint.pprint(tokenized_input['input_ids'])\n","pprint.pprint(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sdDyYaFoO-M"},"outputs":[],"source":["# Define preprocess function\n","def tokenize_and_align_labels(examples):\n","  tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","\n","  labels = []\n","  for i, label in enumerate(examples[f\"ner_tags\"]):\n","    word_ids = tokenized_inputs.word_ids(batch_index=i)\n","    previous_word_idx = None\n","    label_ids = []\n","\n","    for word_idx in word_ids:\n","      if word_idx is None:\n","        label_ids.append(-100)\n","      elif word_idx != previous_word_idx:\n","        # Only label the first token of a given word\n","        label_ids.append(label[word_idx])\n","      else:\n","        label_ids.append(-100)\n","\n","      previous_word_idx = word_idx\n","    labels.append(label_ids)\n","\n","\n","  tokenized_inputs[\"labels\"] = labels\n","\n","  return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zEGex3OpuZt"},"outputs":[],"source":["# Apply preprocessing to every instance in the dataset\n","tokenized_wnut = wnut.map(tokenize_and_align_labels, batched = True,)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yTi6GvUrMzV"},"outputs":[],"source":["# Create a batch of examples, with dynamic padding. Use the appropriate collator function\n","from transformers import DataCollatorForTokenClassification\n","\n","data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"1HRsipJYrZE8"},"source":["# Evaluate\n","We require monitoring a metric during training to see how well our model is doing. We use the evaluate library to load an evaluation metric - we use the seqeval metric. seqeval produces precision, recall, F1 score and accuracy.\n","\n","With the loss metric defined, we must define a function that takes model predictions and labels and computes the loss metric. This is usually called the compute_metrics function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVF5UQ8orU6T"},"outputs":[],"source":["import evaluate\n","\n","seqeval = evaluate.load(\"seqeval\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFz5uX0I4K8n"},"outputs":[],"source":["import numpy as np\n","\n","labels = [label_list[i] for i in example[f\"ner_tags\"]]\n","\n","def compute_metrics(p):\n","  predictions, labels = p\n","  predictions = np.argmax(predictions, axis = 2)\n","\n","  true_predictions = [\n","      [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","      for prediction, label in zip(predictions, labels)\n","  ]\n","  true_labels = [\n","      [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","      for prediction, label in zip(predictions, labels)\n","  ]\n","\n","  results = seqeval.compute(predictions=true_predictions, references=true_labels)\n","\n","  return {\n","      \"precision\": results[\"overall_precision\"],\n","      \"recall\": results[\"overall_recall\"],\n","      'f1': results[\"overall_f1\"],\n","      \"accuracy\": results[\"overall_accuracy\"],\n","  }"]},{"cell_type":"markdown","metadata":{"id":"MzCUJ4u45VgD"},"source":["# Train using the Trainer API\n","\n","The main training steps are:\n","\n","1. Define training hyperparameters using a model specific `TrainingArguments` function. At the end of each epoch, the Trainer will evaluate the defined loss metric and save the training checkpoint.\n","\n","2. Pass the training arguments to a Trainer function alongside the **model**, **dataset**, **tokenizer**, **data collator** and **compute metrics**\n","\n","3. Call train() to finetune the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lzwVKzH5RcC"},"outputs":[],"source":["# Before training, we have to create a map of expected ids to labels with id2label and label2id\n","id2label = {\n","    0: \"O\",\n","    1: \"B-corporation\",\n","    2: \"I-corporation\",\n","    3: \"B-creative-work\",\n","    4: \"I-creative-work\",\n","    5: \"B-group\",\n","    6: \"I-group\",\n","    7: \"B-location\",\n","    8: \"I-location\",\n","    9: \"B-person\",\n","    10: \"I-person\",\n","    11: \"B-product\",\n","    12: \"I-product\",\n","}\n","label2id = {\n","    \"O\": 0,\n","    \"B-corporation\": 1,\n","    \"I-corporation\": 2,\n","    \"B-creative-work\": 3,\n","    \"I-creative-work\": 4,\n","    \"B-group\": 5,\n","    \"I-group\": 6,\n","    \"B-location\": 7,\n","    \"I-location\": 8,\n","    \"B-person\": 9,\n","    \"I-person\": 10,\n","    \"B-product\": 11,\n","    \"I-product\": 12,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F92qA1sK6rEM"},"outputs":[],"source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","\n","model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=len(id2label), id2label= id2label, label2id = label2id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gcGsxfC5evW"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"token_classification_wnut\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=False,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_wnut[\"train\"],\n","    eval_dataset=tokenized_wnut[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMJtDy4v831S"},"outputs":[],"source":["trainer.save_model(\"token_classification_wnut_model\")"]},{"cell_type":"markdown","metadata":{"id":"skuuaBjF8gsa"},"source":["# Inference\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYOHU9vJ81HD"},"outputs":[],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"ner\", model = \"token_classification_wnut_model\")\n","text = \"Michael Faraday was a scientist who lived in England in the 19th century.\"\n","\n","classifier(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MgxKDIxl9GFQ"},"outputs":[],"source":["# Raw Pytorch approach\n","from transformers import AutoTokenizer\n","import torch\n","\n","## Tokenize inputs\n","tokenizer = AutoTokenizer.from_pretrained(\"token_classification_wnut_model\")\n","inputs = tokenizer(text, return_tensors=\"pt\")\n","\n","## Feed inputs to the model and return logits\n","from transformers import AutoModelForTokenClassification\n","\n","model = AutoModelForTokenClassification.from_pretrained(\"token_classification_wnut_model\")\n","with torch.no_grad():\n","  logits = model(**inputs).logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GjBfuLf-rQp"},"outputs":[],"source":["predictions = torch.argmax(logits, dim=2)\n","predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n","predicted_token_class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzfM4Fo9AQ-m"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM+2uTCLGsb9Flj4/FVg1iq","collapsed_sections":["n_Zmee_6m_RY","1HRsipJYrZE8"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
