{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aofp8Qy7hcDAlQkBMMaKrB3v4gfFPmC5","timestamp":1690983977210},{"file_id":"1VQzGDJWUT2t4UNy0O2Y9uDJomoNCKyVq","timestamp":1690983421740},{"file_id":"18pk78C1u2-3DsvG5E3XsVZfHDYBfp5J4","timestamp":1690982780216}],"gpuType":"T4","collapsed_sections":["n84Gl8_qe9Ke"],"authorship_tag":"ABX9TyNc1lrPwpICHPAjSiiWcWNC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Finetune a DistilBERT model on the SQuAD Dataset\n","\n","Task Description: Involves finetuning a model on QA pairs such that a model can answer particular types of questions.\n","\n","Original Tutorial: https://huggingface.co/docs/transformers/tasks/question_answering"],"metadata":{"id":"-YbIBXcexEdo"}},{"cell_type":"code","source":["!pip install -q transformers datasets evaluate accelerate"],"metadata":{"id":"rfiVGwyMxKEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load SQuAD dataset"],"metadata":{"id":"ocHWFEgMxUmn"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","swag = load_dataset(\"squad\", split=\"train[:5000]\")"],"metadata":{"id":"geYs0U6JxRxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["squad = squad.train_test_split(test_size=0.2)"],"metadata":{"id":"s2KnqDXDgh6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Look at the data\n","import pprint\n","pprint.pprint(squad['train'][0])\n","\n","# The text column is our model input\n"],"metadata":{"id":"OtcEya7Kx4-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing\n","## Load Model\n","from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"],"metadata":{"id":"IbtvEgvlyEbI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing\n","We need to create a preprocess function that we will apply to every instance in the dataset. The preprocess function needs to:\n","\n","1. Some examples in a dataset may have a very long context that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the context by setting truncation=\"only_second\".\n","\n","2. Next, map the start and end positions of the answer to the original context by setting return_offset_mapping=True.\n","\n","3. With the mapping in hand, now you can find the start and end tokens of the answer. Use the sequence_ids method to find which part of the offset corresponds to the question and which corresponds to the context."],"metadata":{"id":"sG9COA2JytEJ"}},{"cell_type":"code","source":["def preprocess_function(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=384,\n","        truncation=\"only_second\",\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    answers = examples[\"answers\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offset in enumerate(offset_mapping):\n","        answer = answers[i]\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","        # Find the start and end of the context\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","        # If the answer is not fully inside the context, label it (0, 0)\n","        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # Otherwise it's the start and end token positions\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","    return inputs"],"metadata":{"id":"5wnFWGL0epiW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply preprocessing over entire dataset - batched = True process multiple elements of the datasets\n","tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"],"metadata":{"id":"4pYKGGO7zzYM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_squad['train'][0]"],"metadata":{"id":"kBMLy0Jf0oHo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a batch of examples, with dynamic padding. Create the appropriate collator function\n","from transformers import DefaultDataCollator\n","\n","data_collator = DefaultDataCollator()"],"metadata":{"id":"QOLxnG5Ze2OC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluate\n","\n","We want to create a `compute_metrics` function that monitors a metric during training. For this task, use the accuracy metric."],"metadata":{"id":"n84Gl8_qe9Ke"}},{"cell_type":"code","source":["import evaluate\n","\n","accuracy = evaluate.load(\"accuracy\")"],"metadata":{"id":"stvQKOl0fEBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)"],"metadata":{"id":"ZIWr_EjmfHo8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train using the Trainer API\n","The main training steps are:\n","\n","1. Define training hyperparameters using a model specific TrainingArguments function. At the end of each epoch, the Trainer will evaluate the defined loss metric and save the training checkpoint.\n","\n","2. Pass the training arguments to a Trainer function alongside the model, dataset, tokenizer, data collator.\n","\n","3. Call train() to finetune the model"],"metadata":{"id":"BjcuQs822Wkv"}},{"cell_type":"code","source":["from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n","\n","model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"],"metadata":{"id":"nNB5hINQ3MLe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"qa_model\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    push_to_hub=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_squad[\"train\"],\n","    eval_dataset=tokenized_squad[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()"],"metadata":{"id":"JInJ6PAd2XyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(\"squad_qa_model\")"],"metadata":{"id":"nm1XQNnI4g6I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# In this case, the tokenizer was not saved automatically, save it manually in the model folder for inference\n","tokenizer.save_pretrained(\"squad_qa_model\", legacy_format=False)"],"metadata":{"id":"MYYZQz3E7oKV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference\n","\n","Use model for inference using PyTorch"],"metadata":{"id":"OXjM0h0C4s_F"}},{"cell_type":"code","source":["question = \"How many programming languages does BLOOM support?\"\n","context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\""],"metadata":{"id":"HY2_zVlo4xy3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","question_answerer = pipeline(\"question-answering\", model=\"squad_qa_model\")\n","question_answerer(question=question, context=context)"],"metadata":{"id":"2qIy0hNGhGXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"squad_qa_model\")\n","inputs = tokenizer(question, context, return_tensors=\"pt\")"],"metadata":{"id":"8brWInxW486o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForQuestionAnswering\n","\n","model = AutoModelForQuestionAnswering.from_pretrained(\"squad_qa_model\")\n","with torch.no_grad():\n","    outputs = model(**inputs)"],"metadata":{"id":"NjkHHQvn5Fr5","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["answer_start_index = outputs.start_logits.argmax()\n","answer_end_index = outputs.end_logits.argmax()"],"metadata":{"id":"nvRw8bBb5qA9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n","tokenizer.decode(predict_answer_tokens)"],"metadata":{"id":"tCnM4J9thMW9"},"execution_count":null,"outputs":[]}]}